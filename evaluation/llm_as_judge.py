import json
from openai import OpenAI
import os
from tqdm import tqdm
import argparse
    

# ================ Evaluate ================
def evaluate_answer(question, gold_answer, predicted_answer):
    system_prompt = (
        "You are an expert evaluator on natural sciences, such as hurricanes, geography, and aging dams. Your task is to assess the predicted answer generated by AI models compared to the gold (reference) answer. "
        "Please evaluate the predicted answer on correctness and completeness."
        "For the given question. Evaluate using two criteria:\n\n"
        "1. Correctness (0 or 1):\n"
        "- Score 1 if the predicted answer is generally accurate and aligns with the key points in the reference answer.\n"
        "- Score 0 if it has factual errors or misrepresents key information.\n\n"
        "2. Completeness (0 or 1):\n"
        "- Score 1 if the predicted answer covers the main points present in the reference answer.\n"
        "- Score 0 if it misses essential information or fails to address the core of the question.\n\n"
        "Output your evaluation in the following JSON format:\n"
        '{"correctness": int, "completeness": int, "explanation": "your brief explanation"}'
    )

    user_prompt = f"Question: {question}, Gold Answer: {gold_answer}, Predicted Answer: {predicted_answer}, please evaluate the predicted answer based on correctness and completeness."


    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-2024-11-20",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0
    )

    reply = response.choices[0].message.content

    try:
        result = json.loads(reply.strip())
    except json.JSONDecodeError:
        print("Failed to parse JSON from model output. Raw output:\n", reply)
        result = {"correctness": -1, "completeness": -1, "explanation": "Parse error"}

    return result


# ================ Load Dataset ================
def load_dataset(data_path):
    if data_path.endswith(".json"):
        with open(data_path, "r") as f:
            return json.load(f)
    else:
        raise ValueError("Unsupported file format. Use JSON or CSV.")
    

# ================ Argument Parser ================
def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate QA model performance.")
    parser.add_argument("--data", type=str, required=True, default="hurricane", choices=['hurricane', 'geography', 'aging_dam', 'hydrology', 'chemistry', 'scifact', 'legalbench'], help="data set.")
    parser.add_argument("--model", type=str, choices=["gpt-4", "gpt-4o", "gpt-3.5-turbo", "deepseek", 'llama3', 'llama4', 'gemma', "qwen"], help="Select llm to get answer.")
    parser.add_argument("--retrieval_method", type=str, required=True, default="hypercube", choices=['hypercube', 'semantic', 'union', 'hipporag', 'lightrag', 'contriever', 'bm25', 'graphrag', 'none', 'nvembed'], help="Retrieval methods.")
    parser.add_argument("--k", type=int, default=5, help="Number of retrieved documents.")
    # parser.add_argument("--data_path", type=str, required=True, help="Path to the QA dataset (JSON or CSV).")

    return parser.parse_args()


# ================ Main Function ================
def main():
    args = parse_args()

    if args.retrieval_method == 'none':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_no_rag_responses.json"
    elif args.retrieval_method == 'bm25':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_bm25_k5_responses.json"
    elif args.retrieval_method == 'contriever':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_contriever_k5_responses.json"
    elif args.retrieval_method == 'graphrag':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_graphrag_k5_responses.json"
    elif args.retrieval_method == 'nvembed':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_nvembed_k5_responses.json"
    elif args.retrieval_method == 'hipporag':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_hipporag_k5_responses.json"
    elif args.retrieval_method == 'lightrag':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_lightrag.json"

    elif args.data == 'scifact' and args.retrieval_method == 'bm25':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_bm25_k{args.k}_responses.json"
    
    elif args.data == 'legalbench' and args.retrieval_method == 'bm25':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_bm25_k{args.k}_responses.json"
    
    elif args.data == 'scifact' or args.data == 'legalbench':
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_{args.retrieval_method}_k{args.k}_responses.json"

    else:
        llm_output_dir = f"output/{args.data}/{args.model}/llm_output_{args.retrieval_method}_latest.json"

    
    # Load dataset
    qa_samples = load_dataset(llm_output_dir)
    num_samples = len(qa_samples)
    
    
    # === Evaluate each pair ===
    total_correct, total_complete = 0, 0
    results = []
    for sample in tqdm(qa_samples):
        question = sample["question"]
        # if args.data == "legalbench" or args.data == "scifact":
        #     gold_answer = sample["answers"]
        # else:
        #     gold_answer = sample["gold_answer"]
        gold_answer = sample["gold_answers"]
        predicted_answer = sample['predicted_answer']
        
        eval_result = evaluate_answer(question, gold_answer, predicted_answer)  
        # print("eval_result:", eval_result)
        sample.update(eval_result)
        results.append(sample)

        total_correct += eval_result["correctness"]
        total_complete += eval_result["completeness"]

    print(f"Total samples: {num_samples}, average correctness is {total_correct/num_samples:.4f}, average total_completenes is {total_complete/num_samples:.4f}")

    # # === Save results to a new file ===
    # with open("some_path.json", "w") as f:
    #     json.dump(results, f, indent=2)
    # print("âœ… Evaluation completed and saved!")


if __name__ == "__main__":
    main()
